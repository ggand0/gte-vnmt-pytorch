{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from masked_cross_entropy import *\n",
    "from attn import Attn\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "from enc_dec import EncoderRNN, DecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.4067\n",
       "-0.1418\n",
       "-0.2261\n",
       "-0.0026\n",
       " 1.0591\n",
       " 0.1637\n",
       "-1.8403\n",
       " 0.1628\n",
       "-0.8616\n",
       "-0.2907\n",
       "[torch.cuda.FloatTensor of size 10 (GPU 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_size = 10\n",
    "epsilon = torch.zeros(z_size).cuda()\n",
    "epsilon.normal_(0, 1) # 0 mean unit variance gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4711\n",
       " 0.0977\n",
       " 1.5382\n",
       " 1.4021\n",
       " 1.7067\n",
       " 0.4583\n",
       "-3.0813\n",
       " 0.7612\n",
       " 0.1530\n",
       "-0.8584\n",
       "[torch.cuda.FloatTensor of size 10 (GPU 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_sigma = Variable(torch.randn(z_size)).cuda()\n",
    "mu = Variable(torch.randn(z_size)).cuda()\n",
    "\n",
    "Variable(epsilon * torch.exp(log_sigma.data*0.5)+mu.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 50])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "n_layers = 3\n",
    "bs = 32\n",
    "enc_hidden_dim = 50\n",
    "vocab_size = 20\n",
    "enc_h = Variable(torch.randn(n_layers, bs, enc_hidden_dim))\n",
    "print(enc_h.size())\n",
    "linear_mu = nn.Linear(enc_hidden_dim, z_size)\n",
    "linear_sigma = nn.Linear(enc_hidden_dim, z_size)\n",
    "\n",
    "# use the last hidden layer's output\n",
    "mu = linear_mu(enc_h[-1].squeeze())\n",
    "log_sigma = linear_sigma(enc_h[-1].squeeze())\n",
    "print(mu.size())\n",
    "print(log_sigma.size())\n",
    "\n",
    "epsilon = torch.zeros(z_size)\n",
    "epsilon.normal_(0, 1) # 0 mean unit variance gaussian\n",
    "z = Variable(epsilon*torch.exp(log_sigma.data*0.5)+mu.data)\n",
    "print(z.size()) # => batch_size x z_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 35\n",
    "all_decoder_outputs = Variable(torch.randn(max_seq_len, bs, vocab_size))\n",
    "# ref: http://pytorch.org/docs/master/_modules/torch/nn/modules/loss.html\n",
    "# Target: :math:`(N)` where each value is `0 <= targets[i] <= C-1`\n",
    "target = Variable(torch.randn(max_seq_len, bs)).long()\n",
    "target.data[:] = 0\n",
    "\n",
    "x = all_decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 32, 20])\n",
      "torch.Size([35, 32])\n",
      "==================================================\n",
      "Variable containing:\n",
      " 120.8209\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 3.4520\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "torch.Size([32, 10])\n",
      "==================================================\n",
      "Variable containing:\n",
      " 2.8358\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "print(all_decoder_outputs.size())\n",
    "print(target.size())\n",
    "loss = 0\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for t in range(max_seq_len):\n",
    "    #print(x[t].size())\n",
    "    #print(target[t].size())\n",
    "    #print(target[t].view(target[t].size()[0], 1).size())\n",
    "    #loss += loss_fn(x[t], target[t].view(target[t].size()[0], 1))\n",
    "    loss += loss_fn(x[t], target[t])\n",
    "    #print(type(loss))\n",
    "    #print(loss)\n",
    "    \n",
    "#CE = nn.CrossEntropyLoss(x, target)\n",
    "#CE = F.cross_entropy(x, target)\n",
    "print('='*50)\n",
    "print(loss)\n",
    "print(loss/float(max_seq_len))\n",
    "\n",
    "\n",
    "#KLD = -0.5 * torch.sum(1 + log_sigma - mu*mu - torch.exp(log_sigma))\n",
    "#KLD /= bs # normalize\n",
    "#print(KLD)\n",
    "tmp = 1 + log_sigma - mu*mu - torch.exp(log_sigma)\n",
    "print(tmp.size()) # => bs x z_size\n",
    "KLD = (-0.5 * torch.sum(1 + log_sigma - mu*mu - torch.exp(log_sigma), 1)).mean()\n",
    "print('='*50)\n",
    "print(KLD)\n",
    "\n",
    "\n",
    "print( (loss + KLD).size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
