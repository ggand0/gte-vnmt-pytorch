{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "# ref: https://github.com/pytorch/text/blob/master/torchtext/datasets/snli.py\n",
    "batch_size = 128\n",
    "inputs = data.Field(lower=False)\n",
    "answers = data.Field(sequential=False)\n",
    "train, dev, test = datasets.SNLI.splits(inputs, answers)\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "            (train, dev, test), batch_size=batch_size, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Dataset.__getattr__ at 0x7f2d5927aca8>\n",
      "<generator object Dataset.__getattr__ at 0x7f2d5927aca8>\n",
      "<generator object Dataset.__getattr__ at 0x7f2d5927aca8>\n",
      "549367\n",
      "9842\n",
      "9824\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(dev.shape)\n",
    "print(test.shape)\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x7f2cf79e4b00>\n",
      "{'premise': ['A', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane.'], 'hypothesis': ['A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition.'], 'label': 'neutral'}\n",
      "================================================================================\n",
      "A person on a horse jumps over a broken down airplane.\n",
      "A person is at a diner, ordering an omelette.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **contradiction**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "A person on a horse jumps over a broken down airplane.\n",
      "A person is outdoors, on a horse.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **entailment**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Children smiling and waving at camera\n",
      "They are smiling at their parents\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **neutral**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Children smiling and waving at camera\n",
      "There are children present\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **entailment**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Children smiling and waving at camera\n",
      "The kids are frowning\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **contradiction**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "A boy is jumping on skateboard in the middle of a red bridge.\n",
      "The boy skates down the sidewalk.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **contradiction**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "A boy is jumping on skateboard in the middle of a red bridge.\n",
      "The boy does a skateboarding trick.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **entailment**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "A boy is jumping on skateboard in the middle of a red bridge.\n",
      "The boy is wearing safety equipment.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **neutral**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background.\n",
      "An older man drinks his juice as he waits for his daughter to get off work.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "label: **neutral**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(train[0])\n",
    "# ref: https://stackoverflow.com/questions/2675028/list-attributes-of-an-object\n",
    "print(train[0].__dict__)\n",
    "\n",
    "def printmd(string):\n",
    "    # ref: https://discuss.analyticsvidhya.com/t/how-to-make-a-text-bold-within-print-statement-in-ipython-notebook/14552/2\n",
    "    display(Markdown(string))\n",
    "\n",
    "def display_sentence(tokens):\n",
    "    \"\"\"\n",
    "    ref: https://stackoverflow.com/questions/493386/how-to-print-without-newline-or-space\n",
    "    \"\"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i == len(tokens)-1:\n",
    "            print(token, end='\\n')\n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "\n",
    "for i in range(1, 10):\n",
    "    print('='*80)\n",
    "    display_sentence(train[i].premise)\n",
    "    display_sentence(train[i].hypothesis)\n",
    "    printmd('label: **%s**'%train[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62996\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "word_count = defaultdict(float)\n",
    "for i in train:\n",
    "    #print(i.premise)\n",
    "    #print(i.hypothesis)\n",
    "    for j in i.premise:\n",
    "        word_count[j] += 1\n",
    "    for j in i.hypothesis:\n",
    "        word_count[j] += 1\n",
    "        \n",
    "print( len(list(word_count.keys())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vocab(self, sentences):\n",
    "\n",
    "        # Build vocabulary\n",
    "        word_counts = collections.Counter(sentences)\n",
    "\n",
    "        # Mapping from index to word\n",
    "        idx_to_word = [x[0] for x in word_counts.most_common()]\n",
    "        idx_to_word = list(sorted(idx_to_word)) + [self.pad_token, self.go_token, self.end_token]\n",
    "\n",
    "        words_vocab_size = len(idx_to_word)\n",
    "\n",
    "        # Mapping from word to index\n",
    "        word_to_idx = {x: i for i, x in enumerate(idx_to_word)}\n",
    "\n",
    "        return words_vocab_size, idx_to_word, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "{\"annotator_labels\": [\"neutral\"], \"captionID\": \"3416050480.jpg#4\", \"gold_label\": \"neutral\", \"pairID\": \"3416050480.jpg#4r1n\", \"sentence1\": \"A person on a horse jumps over a broken down airplane.\", \"sentence1_binary_parse\": \"( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )\", \"sentence1_parse\": \"(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))\", \"sentence2\": \"A person is training his horse for a competition.\", \"sentence2_binary_parse\": \"( ( A person ) ( ( is ( ( training ( his horse ) ) ( for ( a competition ) ) ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) (NN competition))))) (. .)))\"}\n",
      "\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "{\"annotator_labels\": [\"entailment\"], \"captionID\": \"3706019259.jpg#3\", \"gold_label\": \"entailment\", \"pairID\": \"3706019259.jpg#3r5e\", \"sentence1\": \"A foreign family is walking along a dirt path next to the water.\", \"sentence1_binary_parse\": \"( ( A ( foreign family ) ) ( ( is ( ( walking ( along ( a ( dirt path ) ) ) ) ( next ( to ( the water ) ) ) ) ) . ) )\", \"sentence1_parse\": \"(ROOT (S (NP (DT A) (JJ foreign) (NN family)) (VP (VBZ is) (VP (VBG walking) (PP (IN along) (NP (DT a) (NN dirt) (NN path))) (ADVP (JJ next) (PP (TO to) (NP (DT the) (NN water)))))) (. .)))\", \"sentence2\": \"A family walks along a dirt path.\", \"sentence2_binary_parse\": \"( ( A family ) ( ( walks ( along ( a ( dirt path ) ) ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (DT A) (NN family)) (VP (VBZ walks) (PP (IN along) (NP (DT a) (NN dirt) (NN path)))) (. .)))\"}\n",
      "\n",
      "183416\n",
      "##################################################\n",
      "{\"annotator_labels\": [\"neutral\", \"entailment\", \"neutral\", \"neutral\", \"neutral\"], \"captionID\": \"4705552913.jpg#2\", \"gold_label\": \"neutral\", \"pairID\": \"4705552913.jpg#2r1n\", \"sentence1\": \"Two women are embracing while holding to go packages.\", \"sentence1_binary_parse\": \"( ( Two women ) ( ( are ( embracing ( while ( holding ( to ( go packages ) ) ) ) ) ) . ) )\", \"sentence1_parse\": \"(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP are) (VP (VBG embracing) (SBAR (IN while) (S (NP (VBG holding)) (VP (TO to) (VP (VB go) (NP (NNS packages)))))))) (. .)))\", \"sentence2\": \"The sisters are hugging goodbye while holding to go packages after just eating lunch.\", \"sentence2_binary_parse\": \"( ( The sisters ) ( ( are ( ( hugging goodbye ) ( while ( holding ( to ( ( go packages ) ( after ( just ( eating lunch ) ) ) ) ) ) ) ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (DT The) (NNS sisters)) (VP (VBP are) (VP (VBG hugging) (NP (UH goodbye)) (PP (IN while) (S (VP (VBG holding) (S (VP (TO to) (VP (VB go) (NP (NNS packages)) (PP (IN after) (S (ADVP (RB just)) (VP (VBG eating) (NP (NN lunch))))))))))))) (. .)))\"}\n",
      "\n",
      "0\n",
      "{\"annotator_labels\": [\"entailment\", \"entailment\", \"entailment\", \"entailment\", \"entailment\"], \"captionID\": \"3552552910.jpg#0\", \"gold_label\": \"entailment\", \"pairID\": \"3552552910.jpg#0r1e\", \"sentence1\": \"Three elephants, each carrying a group of people, walking through the water.\", \"sentence1_binary_parse\": \"( ( ( ( ( ( Three elephants ) , ) ( each ( carrying ( ( a group ) ( of people ) ) ) ) ) , ) ( walking ( through ( the water ) ) ) ) . )\", \"sentence1_parse\": \"(ROOT (NP (NP (CD Three) (NNS elephants)) (, ,) (S (NP (DT each)) (VP (VBG carrying) (NP (NP (DT a) (NN group)) (PP (IN of) (NP (NNS people)))))) (, ,) (VP (VBG walking) (PP (IN through) (NP (DT the) (NN water)))) (. .)))\", \"sentence2\": \"Three elephants carry people through the water.\", \"sentence2_binary_parse\": \"( ( Three elephants ) ( ( ( carry people ) ( through ( the water ) ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (CD Three) (NNS elephants)) (VP (VBP carry) (NP (NNS people)) (PP (IN through) (NP (DT the) (NN water)))) (. .)))\"}\n",
      "\n",
      "3329\n",
      "##################################################\n",
      "{\"annotator_labels\": [\"neutral\", \"contradiction\", \"contradiction\", \"neutral\", \"neutral\"], \"captionID\": \"2677109430.jpg#1\", \"gold_label\": \"neutral\", \"pairID\": \"2677109430.jpg#1r1n\", \"sentence1\": \"This church choir sings to the masses as they sing joyous songs from the book at a church.\", \"sentence1_binary_parse\": \"( ( This ( church choir ) ) ( ( ( sings ( to ( the masses ) ) ) ( as ( they ( ( sing ( joyous songs ) ) ( from ( ( the book ) ( at ( a church ) ) ) ) ) ) ) ) . ) )\", \"sentence1_parse\": \"(ROOT (S (NP (DT This) (NN church) (NN choir)) (VP (VBZ sings) (PP (TO to) (NP (DT the) (NNS masses))) (SBAR (IN as) (S (NP (PRP they)) (VP (VBP sing) (NP (JJ joyous) (NNS songs)) (PP (IN from) (NP (NP (DT the) (NN book)) (PP (IN at) (NP (DT a) (NN church))))))))) (. .)))\", \"sentence2\": \"The church has cracks in the ceiling.\", \"sentence2_binary_parse\": \"( ( The church ) ( ( has ( cracks ( in ( the ceiling ) ) ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (DT The) (NN church)) (VP (VBZ has) (NP (NP (NNS cracks)) (PP (IN in) (NP (DT the) (NN ceiling))))) (. .)))\"}\n",
      "\n",
      "0\n",
      "{\"annotator_labels\": [\"entailment\", \"entailment\", \"entailment\", \"entailment\", \"entailment\"], \"captionID\": \"273248777.jpg#0\", \"gold_label\": \"entailment\", \"pairID\": \"273248777.jpg#0r1e\", \"sentence1\": \"The American footballer in yellow catches the ball whilst under pressure from the payer in white.\", \"sentence1_binary_parse\": \"( ( ( The ( American footballer ) ) ( in ( ( yellow catches ) ( the ball ) ) ) ) ( ( ( whilst ( under pressure ) ) ( from ( ( the payer ) ( in white ) ) ) ) . ) )\", \"sentence1_parse\": \"(ROOT (S (NP (NP (DT The) (JJ American) (NN footballer)) (PP (IN in) (NP (NP (JJ yellow) (NNS catches)) (NP (DT the) (NN ball))))) (VP (VBD whilst) (PP (IN under) (NP (NN pressure))) (PP (IN from) (NP (NP (DT the) (NN payer)) (PP (IN in) (NP (JJ white)))))) (. .)))\", \"sentence2\": \"The American football player in yellow catches the ball while under pressure from the player in white.\", \"sentence2_binary_parse\": \"( ( ( The ( American ( football player ) ) ) ( in yellow ) ) ( ( ( catches ( the ( ball while ) ) ) ( under ( pressure ( from ( ( the player ) ( in white ) ) ) ) ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (NP (DT The) (JJ American) (NN football) (NN player)) (PP (IN in) (NP (NN yellow)))) (VP (VBZ catches) (NP (DT the) (NN ball) (NN while)) (PP (IN under) (NP (NP (NN pressure)) (PP (IN from) (NP (NP (DT the) (NN player)) (PP (IN in) (NP (JJ white)))))))) (. .)))\"}\n",
      "\n",
      "3368\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# Create a simplified dataset containing the entailment label only\n",
    "import os, json\n",
    "\n",
    "root_path = '.data/snli/snli_1.0_entail'\n",
    "train = 'snli_1.0_train.jsonl'\n",
    "dev = 'snli_1.0_dev.jsonl'\n",
    "test = 'snli_1.0_test.jsonl'\n",
    "train_new = 'snli_1.0_train_entail.jsonl'\n",
    "dev_new = 'snli_1.0_dev_entail.jsonl'\n",
    "test_new = 'snli_1.0_test_entail.jsonl'\n",
    "\n",
    "org_files = [train, dev, test]\n",
    "new_files = [train_new, dev_new, test_new]\n",
    "\n",
    "for index, org_file in enumerate(org_files):\n",
    "    print('#'*50)\n",
    "    with open(os.path.join(root_path, org_file)) as f:\n",
    "        lines = f.readlines()\n",
    "        print(lines[0])\n",
    "\n",
    "        lines_new = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i % 100000 == 0:\n",
    "                print(i)\n",
    "            data = json.loads(line)\n",
    "            if data['gold_label'] == 'entailment':\n",
    "                lines_new.append(line)\n",
    "\n",
    "        print(lines_new[100])\n",
    "        print(len(lines_new))\n",
    "        with open(os.path.join(root_path, new_files[index]), 'w') as f_new:\n",
    "            #f_new.write(\"\\n\".join(lines_new))\n",
    "            f_new.write(\"\".join(lines_new))\n",
    "            \n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "{\"annotator_labels\": [\"entailment\"], \"captionID\": \"2267923837.jpg#2\", \"gold_label\": \"entailment\", \"pairID\": \"2267923837.jpg#2r1e\", \"sentence1\": \"Children smiling and waving at camera\", \"sentence1_binary_parse\": \"( Children ( ( ( smiling and ) waving ) ( at camera ) ) )\", \"sentence1_parse\": \"(ROOT (NP (S (NP (NNP Children)) (VP (VBG smiling) (CC and) (VBG waving) (PP (IN at) (NP (NN camera)))))))\", \"sentence2\": \"There are children present\", \"sentence2_binary_parse\": \"( There ( ( are children ) present ) )\", \"sentence2_parse\": \"(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NNS children)) (ADVP (RB present)))))\"}\n",
      "\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.pyenv/versions/3.6.3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "##################################################\n",
      "{\"annotator_labels\": [\"entailment\", \"entailment\", \"entailment\", \"entailment\", \"entailment\"], \"captionID\": \"2407214681.jpg#0\", \"gold_label\": \"entailment\", \"pairID\": \"2407214681.jpg#0r1e\", \"sentence1\": \"Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.\", \"sentence1_binary_parse\": \"( ( ( Two ( young children ) ) ( in ( ( ( ( ( blue jerseys ) , ) ( one ( with ( the ( number 9 ) ) ) ) ) and ) ( one ( with ( the ( number 2 ) ) ) ) ) ) ) ( ( are ( ( ( standing ( on ( ( wooden steps ) ( in ( a bathroom ) ) ) ) ) and ) ( ( washing ( their hands ) ) ( in ( a sink ) ) ) ) ) . ) )\", \"sentence1_parse\": \"(ROOT (S (NP (NP (CD Two) (JJ young) (NNS children)) (PP (IN in) (NP (NP (JJ blue) (NNS jerseys)) (, ,) (NP (NP (CD one)) (PP (IN with) (NP (DT the) (NN number) (CD 9)))) (CC and) (NP (NP (CD one)) (PP (IN with) (NP (DT the) (NN number) (CD 2))))))) (VP (VBP are) (VP (VP (VBG standing) (PP (IN on) (NP (NP (JJ wooden) (NNS steps)) (PP (IN in) (NP (DT a) (NN bathroom)))))) (CC and) (VP (VBG washing) (NP (PRP$ their) (NNS hands)) (PP (IN in) (NP (DT a) (NN sink)))))) (. .)))\", \"sentence2\": \"Two kids in numbered jerseys wash their hands.\", \"sentence2_binary_parse\": \"( ( ( Two kids ) ( in ( numbered jerseys ) ) ) ( ( wash ( their hands ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (NP (CD Two) (NNS kids)) (PP (IN in) (NP (JJ numbered) (NNS jerseys)))) (VP (VBP wash) (NP (PRP$ their) (NNS hands))) (. .)))\"}\n",
      "\n",
      "0\n",
      "##################################################\n",
      "{\"annotator_labels\": [\"entailment\", \"entailment\", \"contradiction\", \"entailment\", \"neutral\"], \"captionID\": \"6160193920.jpg#4\", \"gold_label\": \"entailment\", \"pairID\": \"6160193920.jpg#4r1e\", \"sentence1\": \"A woman with a green headscarf, blue shirt and a very big grin.\", \"sentence1_binary_parse\": \"( ( ( A woman ) ( with ( ( ( ( ( a ( green headscarf ) ) , ) ( blue shirt ) ) and ) ( a ( ( very big ) grin ) ) ) ) ) . )\", \"sentence1_parse\": \"(ROOT (NP (NP (DT A) (NN woman)) (PP (IN with) (NP (NP (DT a) (JJ green) (NN headscarf)) (, ,) (NP (JJ blue) (NN shirt)) (CC and) (NP (DT a) (ADJP (RB very) (JJ big)) (NN grin)))) (. .)))\", \"sentence2\": \"The woman is very happy.\", \"sentence2_binary_parse\": \"( ( The woman ) ( ( is ( very happy ) ) . ) )\", \"sentence2_parse\": \"(ROOT (S (NP (DT The) (NN woman)) (VP (VBZ is) (ADJP (RB very) (JJ happy))) (. .)))\"}\n",
      "\n",
      "0\n",
      "21953\n",
      "[('a', 501616.0), ('.', 332296.0), ('the', 154521.0), ('in', 137321.0), ('is', 126816.0), ('man', 91543.0), ('on', 80077.0), ('are', 70951.0), ('and', 70712.0), ('of', 65593.0), ('with', 59060.0), ('people', 48441.0), ('woman', 46505.0), ('two', 44003.0), (',', 39191.0), ('to', 31620.0), ('at', 30679.0), ('wearing', 29053.0), ('an', 28078.0), ('shirt', 22009.0), ('young', 21177.0), ('outside', 20798.0), ('playing', 20736.0), ('men', 20727.0), ('his', 20338.0), ('girl', 19899.0), ('white', 19637.0), ('boy', 19544.0), ('while', 19205.0), ('black', 18756.0), ('dog', 18014.0), ('sitting', 17825.0), ('there', 17365.0), ('standing', 17293.0), ('blue', 16827.0), ('person', 16201.0), ('group', 15255.0), ('red', 15131.0), ('walking', 14371.0), ('street', 13552.0), ('her', 13086.0), ('holding', 12940.0), ('down', 12926.0), ('-', 12625.0), ('front', 12599.0), ('child', 11977.0), ('water', 11651.0), ('women', 11337.0), ('by', 11281.0), ('one', 11054.0), ('three', 10905.0), ('looking', 9581.0), ('children', 9367.0), ('for', 9237.0), ('some', 9230.0), ('up', 8979.0), ('near', 8366.0), ('as', 7944.0), ('green', 7888.0), ('has', 7870.0), ('their', 7798.0), ('little', 7654.0), ('other', 7623.0), ('large', 6971.0), ('yellow', 6918.0), ('ball', 6848.0), ('riding', 6781.0), ('through', 6762.0), ('next', 6675.0), ('from', 6664.0), ('hat', 6631.0), ('running', 6532.0), ('brown', 6473.0), ('dressed', 6370.0), ('building', 6323.0), ('outdoors', 6182.0), ('another', 6182.0), ('out', 6086.0), ('into', 5979.0), ('small', 5844.0), ('over', 5659.0), ('girls', 5625.0), ('around', 5533.0), ('beach', 5521.0), ('bike', 5433.0), ('crowd', 5415.0), ('something', 5408.0), ('stands', 5378.0), ('dogs', 5374.0), ('working', 5289.0), ('jacket', 5206.0), (\"'\", 5157.0), ('picture', 4948.0), ('field', 4898.0), ('jumping', 4859.0), ('hair', 4776.0), ('together', 4754.0), ('behind', 4680.0), ('table', 4672.0), ('sits', 4611.0)]\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "import re, collections, operator, pickle, json\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''\n",
    "    data_reader.tokenize('a#b')\n",
    "    ['a', '#', 'b']\n",
    "    '''\n",
    "    return [x.strip().lower() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "root_path = '.data/snli/snli_1.0_entail'\n",
    "train_new = 'snli_1.0_train_entail.jsonl'\n",
    "dev_new = 'snli_1.0_dev_entail.jsonl'\n",
    "test_new = 'snli_1.0_test_entail.jsonl'\n",
    "new_files = [train_new, dev_new, test_new]\n",
    "\n",
    "word_counts = collections.defaultdict(float)\n",
    "\n",
    "for index, new_file in enumerate(new_files):\n",
    "    print('#'*50)\n",
    "    with open(os.path.join(root_path, new_file)) as f:\n",
    "        lines = f.readlines()\n",
    "        print(lines[1])\n",
    "    \n",
    "        lines_new = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i % 100000 == 0:\n",
    "                print(i)\n",
    "            data = json.loads(line)\n",
    "            if data['gold_label'] == 'entailment':\n",
    "                tokens1 = tokenize(data['sentence1'])\n",
    "                tokens2 = tokenize(data['sentence2'])\n",
    "                for token in tokens1:\n",
    "                    word_counts[token] += 1\n",
    "                for token in tokens2:\n",
    "                    word_counts[token] += 1\n",
    "                #lines_new.append(line)\n",
    "        \n",
    "\n",
    "print( len(list(word_counts.keys())) )\n",
    "sorted_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_counts[:100])\n",
    "with open(os.path.join(root_path, 'word_counts.dat'), 'wb') as f:\n",
    "    pickle.dump(sorted_counts, f)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21953\n",
      "[('carryout', 1.0), ('cultures', 1.0), ('dealt', 1.0), ('fertilizing', 1.0), ('fertilizering', 1.0), ('laws', 1.0), ('humanitarian', 1.0), ('possing', 1.0), ('scultupres', 1.0), ('powell', 1.0), ('grafffiti', 1.0), ('foils', 1.0), ('tease', 1.0), ('shucker', 1.0), ('pwople', 1.0), ('airlifted', 1.0), ('mik', 1.0), ('ruckus', 1.0), ('88', 1.0), ('tokyo', 1.0), ('foundtain', 1.0), ('charts', 1.0), ('parasailed', 1.0), ('expectant', 1.0), ('pregnancy', 1.0), ('performnef', 1.0), ('costruction', 1.0), ('doord', 1.0), ('brige', 1.0), ('listener', 1.0), ('aquestrian', 1.0), ('unfunny', 1.0), ('bodily', 1.0), ('cleft', 1.0), ('paddleboarding', 1.0), ('functions', 1.0), ('regard', 1.0), ('accurately', 1.0), ('fail', 1.0), ('tangle', 1.0), ('readily', 1.0), ('sipped', 1.0), ('watersking', 1.0), ('teeshirt', 1.0), ('performance3', 1.0), ('crucial', 1.0), ('tripoli', 1.0), ('libya', 1.0), ('smilingly', 1.0), ('sterring', 1.0)]\n",
      "##################################################\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.pyenv/versions/3.6.3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n"
     ]
    }
   ],
   "source": [
    "import fileinput\n",
    "\n",
    "with open(os.path.join(root_path, 'word_counts.dat'), 'rb') as f:\n",
    "    word_count = pickle.load(f)\n",
    "print( len(list(word_counts.keys())) )\n",
    "\n",
    "sorted_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=False)\n",
    "print(sorted_counts[:50])\n",
    "\n",
    "\n",
    "UNK_TOKEN = 'UNK'\n",
    "# ref: https://stackoverflow.com/questions/17140886/how-to-search-and-replace-text-in-a-file-using-python\n",
    "for index, new_file in enumerate(new_files):\n",
    "    print('#'*50)\n",
    "    \n",
    "    with open(os.path.join(root_path, new_file), 'r') as file :\n",
    "        #filedata = file.read()\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    lines_reduced = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        data = json.loads(line)\n",
    "        \n",
    "        #print(type(json.dumps(data)))\n",
    "        \n",
    "        for j, token in enumerate(sorted_counts[:10000]):\n",
    "            tokens1 = tokenize(data['sentence1'])\n",
    "            tokens2 = tokenize(data['sentence2'])\n",
    "            #if token[0] in tokens1 or token[0] in tokens2:\n",
    "            #    data['sentence1'] = data['sentence1'].replace(token[0], UNK_TOKEN)\n",
    "            #    data['sentence2'] = data['sentence2'].replace(token[0], UNK_TOKEN)\n",
    "            \n",
    "                \n",
    "            replaced1 = [UNK_TOKEN if x!=UNK_TOKEN and token[0] == x else x for x in tokens1]\n",
    "            replaced2 = [UNK_TOKEN if x!=UNK_TOKEN and token[0] == x else x for x in tokens2]\n",
    "            data['sentence1'] = \" \".join(replaced1)\n",
    "            data['sentence2'] = \" \".join(replaced2)\n",
    "            #if token[0] in tokens1 or token[0] in tokens2:\n",
    "            #    print(replaced1)\n",
    "            #    print(replaced2)\n",
    "            #    print(data['sentence1'])\n",
    "            #    print(data['sentence2'])\n",
    "                \n",
    "                    \n",
    "        lines_reduced.append(json.dumps(data))\n",
    "            \n",
    "    with open(os.path.join(root_path, '%s_reduced'%new_file), 'w') as f:\n",
    "        f.write(\"\\n\".join(lines_reduced))\n",
    "                         \n",
    "        # Replace the target string\n",
    "    #   filedata = filedata.replace(token[0], UNK_TOKEN)\n",
    "    #   Write the file out again\n",
    "    #with open(os.path.join(root_path, '%s_reduced'%new_file), 'w') as file:\n",
    "    #    file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
